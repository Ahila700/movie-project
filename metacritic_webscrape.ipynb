{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd \n",
    "import json \n",
    "import numpy as np\n",
    "import pprint\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a header file to \n",
    "header = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice webscrape with a single game to use the same code in an extended loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a seperate url to webscrape all data from a single game so i can loop it\n",
    "# for a large set of game\n",
    "\n",
    "\n",
    "urlr = 'https://www.metacritic.com/game/xbox-one/red-dead-redemption-2'\n",
    "response_soupr = requests.get(urlr , headers = header)\n",
    "soupr = BeautifulSoup(response_soupr.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Red Dead Redemption 2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I went to the website to find what I want\n",
    "## then keep going up in class until I find something with a unique title\n",
    "title = soupr.findAll('div', {'class' : \"product_title\"})\n",
    "\n",
    "# now we have a much smaller section to look through and grab the piece of info we ashley care about\n",
    "title[0]('h1')[0].text\n",
    "\n",
    "# i continued this process for each individual topic i wanted to find on the specific page\n",
    "# seperating all of them in individual blocks as they all have a unique method to the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'97'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metascores = soupr.findAll('div', {'class' : \"metascore_wrap highlight_metascore\"})\n",
    "\n",
    "metascores[0].span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'33'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metascore_review = soupr.findAll('span', {'class' : \"count\"})\n",
    "metascore_reviews = metascore_review[0].text.replace('\\n', '').split()\n",
    "metascore_reviews[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.8'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_scores = soupr.findAll('div', {'class' : \"userscore_wrap feature_userscore\"})\n",
    "\n",
    "user_score = user_scores[0].text.replace('\\n', ' ').split()[2]\n",
    "user_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generally favorable reviews'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_review = soupr.findAll('div', {'class' : \"details side_details\"})\n",
    "user_review[0].p.span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_rating = soupr.findAll('li', {'class' : \"summary_detail product_rating\"})\n",
    "content_rating[0].text.replace('\\n', ' ').split()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Action Adventure, Open-World']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publisher = soupr.findAll('li', {'class' : \"summary_detail product_genre\"})\n",
    "[' '.join(publisher[0].text.replace('\\n', ' ').split()[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rockstar Games'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = soupr.findAll('li', {'class' : \"summary_detail developer\"})\n",
    "' '.join(genres[0].text.replace('\\n', ' ').split()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oct 26, 2018'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_date = soupr.findAll('li', {'class' : \"summary_detail release_data\"})\n",
    "' '.join(release_date[0].text.replace('\\n', ' ').split()[2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url and response for the general page that has the hyperlink for all of the games \n",
    "# split up into 9 different pages\n",
    "\n",
    "# urlb = 'https://www.metacritic.com/browse/games/release-date/available/xboxone/metascore?page=0'\n",
    "# response_soup = requests.get(urlb, headers = header)\n",
    "# soup = BeautifulSoup(response_soup.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new empty list to store all hyperlinks\n",
    "all_hyperlink = []\n",
    "\n",
    "# create a function to get all game links with parameters of the url and header\n",
    "def get_game_links_from_site(url, header):\n",
    "    response_soup = requests.get(url, headers = header)\n",
    "    soup = BeautifulSoup(response_soup.text, 'html.parser')\n",
    "\n",
    "    # game tablle is the block of information for all of the games\n",
    "    game_table = soup.find_all('ol', class_=\"list_products list_product_condensed\")\n",
    "    \n",
    "    # game is the box of data that has individual games in the game_table\n",
    "    for game in game_table:\n",
    "        \n",
    "        # create a new variable to loop through to obtain the hyperlink in each block\n",
    "        game_links_html = game.find_all('a', href=True)\n",
    "        \n",
    "        # this block is to test if the code is reading the url by giving it a sleep timer \n",
    "        # so it can stop for 5 seconds and continue again afterwards\n",
    "        # if it goes through this block it will just call to the individual block again\n",
    "        if game_links_html == []:\n",
    "            time.sleep(5)\n",
    "            game_links_html = game.find_all('a', href=True)\n",
    "        \n",
    "        # it continues onto this whether it goes into the if statement or not\n",
    "        # will look through each block in game_links_html and appends\n",
    "        # that hyperlink into my list of all hyperlinks\n",
    "        for game_link in game_links_html:\n",
    "            all_hyperlink.append(game_link['href'])\n",
    "           # print(game_link['href'])\n",
    "    return all_hyperlink\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the only thing left is to go through all the pages in the website since there were \n",
    "# 9 pages (page 0 to 8) and the only thing that changes is the page= \n",
    "# I just needed to change the page number and loop throguh it and it will append all game hyperlinks to our list \n",
    "\n",
    "for i in range(0, 9):\n",
    "    url = 'https://www.metacritic.com/browse/games/release-date/available/xboxone/metascore?page={}'.format(i)        \n",
    "    get_game_links_from_site(url, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of dictionaries that will later convert into a dataframe\n",
    "\n",
    "metacritic_dataframe_list = []\n",
    "\n",
    "# create a function to append all the individual items for every game into a dictionary and \n",
    "# appending the dictionary into the list. Only parameter that will change is the url which\n",
    "# is just coming from the list of urls from the previous function\n",
    "\n",
    "def get_data_from_each_game (url):   \n",
    "    \n",
    "    response_soup = requests.get(url, headers = header)\n",
    "    soup = BeautifulSoup(response_soup.text, 'html.parser')\n",
    "\n",
    "    title = soup.findAll('div', {'class' : \"product_title\"})\n",
    "\n",
    "    metascores = soup.findAll('div', {'class' : \"metascore_wrap highlight_metascore\"})\n",
    "\n",
    "    metascore_review = soup.findAll('span', {'class' : \"count\"})\n",
    "\n",
    "    user_scores = soup.findAll('div', {'class' : \"userscore_wrap feature_userscore\"})\n",
    "\n",
    "    user_review = soup.findAll('div', {'class' : \"details side_details\"})\n",
    "\n",
    "    content_rating = soup.findAll('li', {'class' : \"summary_detail product_rating\"})\n",
    "\n",
    "    genres = soup.findAll('li', {'class' : \"summary_detail product_genre\"})\n",
    "\n",
    "    developer = soup.findAll('li', {'class' : \"summary_detail developer\"})\n",
    "\n",
    "    release_date = soup.findAll('li', {'class' : \"summary_detail release_data\"})\n",
    "\n",
    "    # needed to create a try and exccept here because the  user review count is different \n",
    "    # if there are reviews as opposed to if there arent. so i used a try and except based \n",
    "    # on the error i got and i can output the appropriate dictionary for the gsme\n",
    "    \n",
    "    try:\n",
    "        metacritic_dict = {\n",
    "            'title': title[0].h1.text,\n",
    "            'metacritic_score': metascores[0].span.text,\n",
    "            'metacritic_review_count': metascore_review[0].text.replace('\\n', '').split()[3],\n",
    "            'user_score': user_scores[0].text.replace('\\n', ' ').split()[2],\n",
    "            'user_review_count': user_review[0].p.a.text.split()[0],\n",
    "            'content_rating': content_rating[0].text.replace('\\n', ' ').split()[1],\n",
    "            'developer':' '.join(developer[0].text.replace('\\n', ' ').split()[1:]),\n",
    "            'genres': [' '.join(genres[0].text.replace('\\n', ' ').split()[1:])],\n",
    "            'release_date': ' '.join(release_date[0].text.replace('\\n', ' ').split()[2:])\n",
    "        }\n",
    "    except AttributeError:\n",
    "        metacritic_dict = {\n",
    "            'title': title[0].h1.text,\n",
    "            'metacritic_score': metascores[0].span.text,\n",
    "            'metacritic_review_count': metascore_review[0].text.replace('\\n', '').split()[3],\n",
    "            'user_score': user_scores[0].text.replace('\\n', ' ').split()[2],\n",
    "            'user_review_count': user_review[0].p.span.text,\n",
    "            'content_rating': content_rating[0].text.replace('\\n', ' ').split()[1],\n",
    "            'developer':' '.join(developer[0].text.replace('\\n', ' ').split()[1:]),\n",
    "            'genres': [' '.join(genres[0].text.replace('\\n', ' ').split()[1:])],\n",
    "            'release_date': ' '.join(release_date[0].text.replace('\\n', ' ').split()[2:])\n",
    "        }\n",
    "    \n",
    "    # appending the dictionary to the list\n",
    "    metacritic_dataframe_list.append(metacritic_dict)\n",
    "    \n",
    "    return metacritic_dataframe_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems loading the url in random cases, created a try and catch to catch any errors\n",
    "# but continue past them. stored the errors in its own list that i can run through again\n",
    "# until the list of errors is empty\n",
    "\n",
    "errors = []\n",
    "\n",
    "# goes through all the links in the hyperlinks list and edits the url accordingly then\n",
    "# runs the function for that url to get the games\n",
    "\n",
    "for link in all_hyperlink:\n",
    "    try: \n",
    "        urlr = 'https://www.metacritic.com{}'.format(link)\n",
    "        get_data_from_each_game(urlr)\n",
    "    except IndexError:\n",
    "        errors.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# had to create a second list of errors to catch problems when going through the \n",
    "# original list of errors\n",
    "\n",
    "new_errors = []\n",
    "\n",
    "for link in errors:\n",
    "    try: \n",
    "        urlr = 'https://www.metacritic.com{}'.format(link)\n",
    "        get_data_from_each_game(urlr)\n",
    "    except IndexError as e:\n",
    "        new_errors.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to go through the new_errors list and will keep repeating with the previous code\n",
    "# until there are no errors\n",
    "\n",
    "errors = []\n",
    "\n",
    "for link in new_errors:\n",
    "    try: \n",
    "        urlr = 'https://www.metacritic.com{}'.format(link)\n",
    "        get_data_from_each_game(urlr)\n",
    "    except IndexError as e:\n",
    "        errors.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used pickle to store the lists i created since the function and loops took a long time\n",
    "# to run i didnt want to have to run them over and over. \n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will open up the files i created\n",
    "\n",
    "with open('hyperlink_list', 'rb') as handle:\n",
    "    all_hyperlink = pickle.load(handle)\n",
    "    \n",
    "with open('error_urls', 'rb') as handle:\n",
    "    errors = pickle.load(handle)\n",
    "\n",
    "with open('working_list', 'rb') as handle:\n",
    "    metacritic_dataframe_list = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will store the files\n",
    "\n",
    "with open('hyperlink_list', 'wb') as handle:\n",
    "    pickle.dump(all_hyperlink, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('working_list', 'wb') as handle:\n",
    "    pickle.dump(metacritic_dataframe_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('error_urls', 'wb') as handle:\n",
    "    pickle.dump(errors, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated a dataframe with the list of dictionaries \n",
    "df = pd.DataFrame(metacritic_dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the numbers came back as strings so i had to change it. but in user score and \n",
    "# user review count they had 'tbd or 'No user score yet' respectively so i had to change those\n",
    "# to nan so i could convert everything else into a float/int\n",
    "\n",
    "\n",
    "df = pd.to_numeric(df, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('metacritic_reviews.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
